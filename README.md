 Cloud-Based ETL & Realâ€‘Time Streaming Pipeline for Healthcare Admissions

This repository showcases two major data engineering workflows built using synthetic healthcare admissions data:

             A cloudâ€‘ready ETL pipeline for batch analytics
             A realâ€‘time streaming pipeline using Kafka + Python

Together, they demonstrate how modern healthcare systems ingest, clean, transform, and stream data for analytics, dashboards, and machine learning.

                            Project Overview

This project contains two integrated pipelines:

I - Cloud ETL Pipeline (Batch Processing)

A complete Extractâ€“Transformâ€“Load workflow that processes 1000 synthetic healthcare admission records.

ETL Steps

1. Ingest

Loads raw synthetic healthcare admissions data generated programmatically.

2. Clean
Removes duplicates

Handles missing values

Fixes invalid values

Standardizes data types

Adds derived fields (year, month, elderly flag)

3. Transform
Aggregates metrics by:

Hospital unit

Primary condition

Metrics include:

Admissions count

Average length of stay

Readmission rate

Average cost

4. Load
Outputs:

data_processed/healthcare_admissions_clean.csv

data_processed/healthcare_aggregated_metrics.csv

These are ready for:

Power BI dashboards

Tableau visualizations

Machine learning models

II  Realâ€‘Time Streaming Pipeline â€” Kafka + Python Producer

This module simulates realâ€‘time healthcare admission events flowing through an Apache Kafka pipeline.

It is the first component of a full streaming architecture that will later include:

Spark Structured Streaming

Delta Lake storage

Realâ€‘time dashboards

*******************************************************************************

Realâ€‘Time Pipeline Architecture


Windows Host (Python Producer)
        |
        |  PLAINTEXT_HOST://localhost:29092
        v
+---------------------+
|   Kafka Broker      |
|  (Docker Container) |
+---------------------+
        ^
        |  PLAINTEXT://kafka:9092
        |
+---------------------+
|     Kafka UI        |
|  (Docker Container) |
+---------------------+


---
Components

1. Kafka Cluster (Docker)
       - Zookeeper
       - Kafka Broker
       - Dualâ€‘listener configuration
       - Exposed ports:
            9092 (Docker internal)
            29092 (Windows host)
2. Kafkaâ€‘UI
     A browserâ€‘based interface for inspecting:
       - Brokers
       - Topics
       - Partitions
       - Realâ€‘time messages

3. Python Producer

  Streams 1,000 synthetic healthcare admission events into Kafka.

       - Each event includes:
       - Patient ID
       - Hospital unit
       - Vital signs
       - Primary condition
       - Admission timestamp


How to Run the Streaming Module
1. Start Kafka + Kafkaâ€‘UI
    From the /docker folder:

        docker compose up -d
2. Run the Python Producer
    From the /producer folder:
        python healthcare_producer.py
Expected output:

   Sending 1000 events to topic: healthcare_admissions_stream
   1/1000 Sent: {...}
   2/1000 Sent: {...}
...
3. View Messages in Kafkaâ€‘UI

Open:
   http://localhost:8080
   Navigate to:

   local â†’ Topics â†’ healthcare_admissions_stream â†’ Messages
##  ðŸ“ Folder Structure

cloud_streaming_healthcare/
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ healthcare_producer.py
â”œâ”€â”€ consumer/
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ generate_healthcare_data.py
â”œâ”€â”€ data_raw/
â”œâ”€â”€ data_processed/
â””â”€â”€ README.md


=> Project Status
Component	               Status
Batch ETL pipeline	       Completed
Kafka cluster	               Running
Kafkaâ€‘UI	                       Connected
Python producer	               Streaming 1000 events
Spark Structured Streaming     Next step
Delta Lake storage	       Planned
Realâ€‘time dashboard	       Planned


```






